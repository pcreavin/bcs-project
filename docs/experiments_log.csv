
**Day 2:**
- [ ] Enhance `scripts/compare_ablation.py`:
  - [ ] Add per-class F1 scores to comparison table
  - [ ] Generate LaTeX table output for milestone doc
  - [ ] Create visualization: bar chart comparing macro-F1 across methods
- [ ] Create milestone document template (`docs/milestone.md`):
  ```markdown
  # Cattle Body Condition Scoring from Photos
  ## Team Members: [Names]

  ## Motivation
  ## Method
  ## Preliminary Experiments
  ## Error Analysis
  ## Next Steps
  ## Contributions
  ```

**Deliverables:** Clean repo, experiment tracking setup, milestone template, enhanced comparison script

---

## **DAYS 3-5: Experiments & Baselines** (Jan 3-5)

### **Person A: Data & Evaluation**
**Day 3:**
- [ ] Implement baseline models (`src/models/baselines.py`):
  ```python
  # 1. Extract features from frozen EfficientNet
  # 2. Logistic Regression on frozen features
  # 3. SVM on frozen features (linear & RBF kernel)

  def extract_frozen_features(model, dataloader, device):
      """Extract features from frozen pretrained model."""
      features, labels = [], []
      model.eval()
      with torch.no_grad():
          for x, y in dataloader:
              x = x.to(device)
              feat = model.forward_features(x)  # timm feature extraction
              features.append(feat.cpu().numpy())
              labels.append(y.numpy())
      return np.concatenate(features), np.concatenate(labels)

  # Train LogisticRegression, SVC from sklearn
  # Evaluate with same metrics as deep models
  ```
- [ ] Run baselines:
  ```bash
  python scripts/run_baselines.py  # New script
  # Should produce: outputs/baseline_logreg/, outputs/baseline_svm/
  ```

**Day 4:**
- [ ] Create error analysis notebook (`notebooks/error_analysis.ipynb`):
  - [ ] Load best model from each ablation
  - [ ] Identify top-confused class pairs
  - [ ] Visualize 12-24 misclassified examples
  - [ ] Analyze underweight (class 0) recall specifically
  - [ ] Generate plots: confusion matrices, per-class recall bars

**Day 5:**
- [ ] Aggregate all results into summary:
  - [ ] Create `results/summary_table.csv` with all experiments
  - [ ] Generate comparison plots:
    - [ ] Macro-F1 comparison (bar chart)
    - [ ] Underweight recall comparison
    - [ ] Per-class recall heatmap

**Deliverables:** Baseline models trained, error analysis notebook, summary tables/plots

---

### **Person B: Modeling & Training**
**Day 3:**
- [ ] Run first ablation experiment (recommend starting with `full`):
  ```bash
  python -m src.train.train --config configs/ablation_full.yaml
  # Monitor: Should complete without errors, save best_model.pt, metrics.json, confusion_matrix.png
  ```
- [ ] Document runtime, GPU usage, final metrics

**Day 4:**
- [ ] Run second ablation (`head_only`):
  ```bash
  python -m src.train.train --config configs/ablation_head_only.yaml
  ```
- [ ] Run third ablation (`last_block`):
  ```bash
  python -m src.train.train --config configs/ablation_last_block.yaml
  ```
- [ ] Monitor training:
  - [ ] Check for overfitting (train vs val loss gap)
  - [ ] Verify early stopping triggers appropriately
  - [ ] Note any training instabilities

**Day 5:**
- [ ] Run final ablation (`scratch`):
  ```bash
  python -m src.train.train --config configs/ablation_scratch.yaml
  ```
- [ ] Hyperparameter quick sweep (if time):
  - [ ] Try different learning rates (1e-4, 3e-4, 1e-3) on best method
  - [ ] Document results in `docs/experiments.md`

**Deliverables:** 4 ablation experiments complete, 3+ successful runs with all artifacts, training logs

---

### **Person C: MLOps & Writing**
**Day 3:**
- [ ] Set up automated results aggregation:
  ```bash
  # Create scripts/aggregate_results.py
  # - Scans outputs/ directory
  # - Extracts metrics from all experiments
  # - Creates consolidated results.csv
  # - Updates comparison tables
  ```
- [ ] Create figure generation script (`scripts/generate_figures.py`):
  ```python
  # Generate:
  # - Comparison bar charts (macro-F1, underweight recall)
  # - Confusion matrix side-by-side for top 2 methods
  # - Training curves (if tracking loss)
  ```

**Day 4:**
- [ ] Draft milestone document sections:
  - [ ] **Motivation** (1-2 paragraphs): Problem, setting, impact
  - [ ] **Method** (1 page):
    - [ ] EfficientNet-B0 backbone
    - [ ] Transfer learning strategies (4 modes)
    - [ ] Baseline methods (LogReg, SVM)
    - [ ] Evaluation metrics
- [ ] Create tables/figures placeholders

**Day 5:**
- [ ] Draft **Preliminary Experiments** section:
  - [ ] Experiment setup (data splits, hyperparameters)
  - [ ] Results table (all methods with metrics)
  - [ ] Key findings (which method works best, why)

**Deliverables:** Results aggregation working, milestone draft (Motivation + Method + Preliminary Experiments)

---

## **DAYS 6-8: Analysis & Refinement** (Jan 6-8)

### **Person A: Data & Evaluation**
**Day 6:**
- [ ] Complete error analysis notebook:
  - [ ] Top 5 most confused class pairs with examples
  - [ ] Underweight (class 0) failure analysis
  - [ ] Visual patterns in misclassified images

**Day 7:**
- [ ] Write error analysis summary for milestone:
  - [ ] Key insights from confusion matrix
  - [ ] Patterns in misclassifications
  - [ ] Implications for model improvement

**Day 8:**
- [ ] Finalize evaluation documentation:
  - [ ] Update `docs/data_readme.md` with final metrics schema
  - [ ] Create `docs/metrics_explanation.md` (what each metric means, why we use it)

**Deliverables:** Complete error analysis, written summary, updated docs

---

### **Person B: Modeling & Training**
**Day 6:**
- [ ] Review all experiment results:
  - [ ] Verify all experiments completed successfully
  - [ ] Check for anomalies (overfitting, unstable training)
  - [ ] Document best hyperparameters for each method

**Day 7:**
- [ ] Run additional experiments if needed:
  - [ ] Re-run best method with different seed (verify stability)
  - [ ] Quick hyperparameter tuning on best method
- [ ] Update `docs/experiments.md` with:
  - [ ] All experiment configs
  - [ ] Results summary
  - [ ] Runtime information

**Day 8:**
- [ ] Model checkpoint verification:
  - [ ] Test loading saved models:
    ```python
    model.load_state_dict(torch.load('outputs/.../best_model.pt'))
    ```
  - [ ] Verify model can make predictions on new data
  - [ ] Document checkpoint format

**Deliverables:** All experiments documented, stable results, verified checkpoints

---

### **Person C: MLOps & Writing**
**Day 6:**
- [ ] Complete milestone document:
  - [ ] **Error Analysis** section (with Person A's findings)
  - [ ] **Next Steps** section:
    - [ ] Planned improvements
    - [ ] Remaining experiments
    - [ ] Final model selection strategy

**Day 7:**
- [ ] Create final figures for milestone:
  - [ ] Comparison table (all methods)
  - [ ] Confusion matrix (best method)
  - [ ] Error analysis visualizations
- [ ] **Contributions** section:
  - [ ] Person A: [list contributions]
  - [ ] Person B: [list contributions]
  - [ ] Person C: [list contributions]

**Day 8:**
- [ ] Final milestone polish:
  - [ ] Proofread entire document
  - [ ] Verify all figures/tables referenced correctly
  - [ ] Check formatting (3 pages max, excluding references)
  - [ ] Ensure all team member names included
  - [ ] Verify title matches proposal

**Deliverables:** Complete milestone document with all sections, figures, tables

---

## **DAYS 9-10: Final Review & Submission** (Jan 9-10)

### **All Team Members**

**Day 9:**
- [ ] **Group review session:**
  - [ ] Read milestone document together
  - [ ] Verify all results are accurate
  - [ ] Check that contributions section is fair
  - [ ] Test that code runs end-to-end
  - [ ] Verify all experiments are reproducible

**Day 10:**
- [ ] **Final checks:**
  - [ ] All code committed and pushed
  - [ ] All experiment outputs saved in `outputs/`
  - [ ] Milestone document exported to PDF
  - [ ] Verify page count (‚â§3 pages + references)
  - [ ] Submit milestone

---

## üìä **CRITICAL DELIVERABLES CHECKLIST**

### **Code & Experiments:**
- [ ] 4 ablation experiments complete (scratch, head_only, last_block, full)
- [ ] 2 baseline models (Logistic Regression, SVM)
- [ ] All experiments save: `best_model.pt`, `config.yaml`, `metrics.json`, `confusion_matrix.png`
- [ ] Results comparison table generated
- [ ] Error analysis notebook complete

### **Documentation:**
- [ ] Updated `docs/data_readme.md`
- [ ] Updated `docs/experiments.md` with all runs
- [ ] Milestone document (3 pages + references)
  - [ ] Motivation
  - [ ] Method
  - [ ] Preliminary Experiments
  - [ ] Error Analysis
  - [ ] Next Steps
  - [ ] Contributions

### **Repository:**
- [ ] All code pushed to git
- [ ] `.gitignore` properly configured
- [ ] Experiment tracking set up
- [ ] Automation scripts working (`run_ablation.sh`, `compare_ablation.py`)

---

## üéØ **SUCCESS CRITERIA**

By Day 10, you should have:
1. ‚úÖ At least 3 successful deep learning experiments (4 preferred)
2. ‚úÖ 2 baseline models for comparison (CS229 requirement)
3. ‚úÖ Comprehensive metrics for all methods (macro-F1, underweight recall, per-class metrics)
4. ‚úÖ Error analysis identifying key failure modes
5. ‚úÖ Complete milestone document ready for submission
6. ‚úÖ Reproducible codebase (anyone can run experiments)

---

## üìû **COMMUNICATION PROTOCOL**

- Daily standup (15 min): Progress, blockers, help needed
- Shared document: Live milestone draft (Google Docs/Notion)
- Git workflow: Branch per feature, merge to main daily
- Results sharing: Use shared `outputs/` directory, tag experiments clearly

---

## ‚ö†Ô∏è **RISK MITIGATION**

**If experiments fail:**
- Person B: Debug immediately, try smaller batch size, fewer epochs
- Person C: Document failures in milestone (still valuable learning)

**If running out of time:**
- Prioritize: 2-3 best experiments over 4 mediocre ones
- Focus: Best method + 1 baseline minimum

**If team member falls behind:**
- Redistribute tasks immediately
- Focus on milestone document completion (most critical)

---

## üöÄ **QUICK START COMMANDS**

```bash
# Day 1: Validate setup
python test_local_setup.py  # (if you created this)

# Day 3+: Run experiments
bash scripts/run_ablation.sh

# Day 5+: Compare results
python scripts/compare_ablation.py

# Day 7+: Generate figures
python scripts/generate_figures.py  # (if created)
```

This plan should get you to a strong milestone. Adjust timelines based on your team's availability and priorities.